{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e31cf-7fff-4d4b-ae38-e55f586bb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.14.5\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # Revolution Risk Predictor - Model Development\n",
    "# \n",
    "# This notebook handles model training, evaluation, and selection for the revolution risk prediction.\n",
    "\n",
    "# ## Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, average_precision_score, \n",
    "                             confusion_matrix, classification_report, RocCurveDisplay,\n",
    "                             PrecisionRecallDisplay)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ## Load or Generate Data\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\"Generate synthetic country data with revolutionary event labels\"\"\"\n",
    "    countries_data = {\n",
    "        'country': ['France', 'Germany', 'UK', 'Italy', 'Spain', 'Poland',\n",
    "                   'Ukraine', 'Turkey', 'Egypt', 'Tunisia', 'Brazil', 'Argentina',\n",
    "                   'Mexico', 'USA', 'Canada', 'Australia', 'Tasmania', 'Japan',\n",
    "                   'South Korea', 'China', 'India', 'Pakistan', 'Nigeria', 'South Africa',\n",
    "                   'Malaysia', 'Thailand', 'Indonesia', 'Vietnam', 'Philippines', 'Myanmar'],\n",
    "        'gdp': [41464, 48560, 42724, 34260, 29875, 17319, 3985, 9061, 3618, 3440,\n",
    "               6796, 10639, 9946, 65280, 46194, 51692, 45000, 40113, 31846, 10500,\n",
    "               2100, 1193, 2028, 6040, 11372, 7274, 4294, 2823, 3595, 1263],\n",
    "        'unemployment': [7.9, 3.0, 3.7, 9.7, 13.8, 3.4, 9.8, 10.6, 7.3, 15.2,\n",
    "                        11.6, 8.5, 3.3, 3.7, 5.3, 5.1, 6.2, 2.4, 3.7, 4.8, 7.1,\n",
    "                        6.3, 9.8, 28.5, 3.7, 1.2, 6.3, 2.3, 5.1, 1.9],\n",
    "        'youth_pct': [17.8, 15.1, 17.5, 15.1, 14.7, 18.3, 19.1, 25.6, 33.3, 29.4,\n",
    "                     27.4, 24.9, 26.3, 19.0, 16.0, 19.0, 18.5, 14.5, 19.0, 17.2,\n",
    "                     27.0, 35.0, 42.5, 29.5, 24.8, 17.8, 27.3, 23.0, 31.9, 28.3],\n",
    "        'internet_pct': [85.6, 89.7, 94.9, 74.5, 87.1, 82.9, 64.3, 71.0, 57.3, 66.3,\n",
    "                        70.7, 79.9, 65.8, 87.3, 91.0, 88.2, 85.0, 93.0, 95.1, 61.2,\n",
    "                        45.0, 35.1, 42.0, 56.2, 89.6, 77.8, 73.7, 70.3, 60.1, 44.8],\n",
    "        'polity': [8, 10, 10, 10, 8, 10, 7, -3, -3, 7, 8, 8, 8, 8, 10, 10, 10, 10,\n",
    "                  10, -7, 9, 5, 7, 9, 8, -2, 8, -7, 7, -2]\n",
    "    }\n",
    "    \n",
    "    df_countries = pd.DataFrame(countries_data)\n",
    "    \n",
    "    # Generate monthly data for the past 3 years\n",
    "    months = pd.date_range(\"2021-01-01\", \"2023-12-01\", freq='MS')\n",
    "    rows = []\n",
    "    \n",
    "    for _, country_row in df_countries.iterrows():\n",
    "        country = country_row['country']\n",
    "        base_values = country_row.to_dict()\n",
    "        \n",
    "        for i, d in enumerate(months):\n",
    "            row = base_values.copy()\n",
    "            row['date'] = d\n",
    "            row['year'] = d.year\n",
    "            row['month'] = d.month\n",
    "            \n",
    "            # Add realistic fluctuations to economic indicators\n",
    "            row['gdp'] = row['gdp'] * (1 + np.random.normal(0, 0.01))\n",
    "            row['unemployment'] = max(1, row['unemployment'] + np.random.normal(0, 0.5))\n",
    "            row['youth_pct'] = max(5, min(70, row['youth_pct'] + np.random.normal(0, 0.2)))\n",
    "            row['internet_pct'] = max(1, min(100, row['internet_pct'] + np.random.normal(0, 0.3)))\n",
    "            row['polity'] = max(-10, min(10, row['polity'] + np.random.normal(0, 0.1)))\n",
    "            \n",
    "            # Calculate a realistic risk score based on known factors\n",
    "            risk_factors = (\n",
    "                (100 - row['polity']) * 0.1 +  # Lower polity = higher risk\n",
    "                row['unemployment'] * 0.15 +   # Higher unemployment = higher risk\n",
    "                row['youth_pct'] * 0.05 +      # More youth = slightly higher risk\n",
    "                (100 - row['internet_pct']) * 0.02 +  # Less internet access = slightly higher risk\n",
    "                (40000 / (row['gdp'] + 1000)) * 0.5  # Lower GDP = higher risk\n",
    "            )\n",
    "            \n",
    "            # Add some noise and time-based variation\n",
    "            time_factor = np.sin(i / 6.0) * 0.5  # Seasonal effect\n",
    "            risk_score = risk_factors + np.random.normal(0, 0.5) + time_factor\n",
    "            \n",
    "            # Ensure we have a good mix of both classes in the training data\n",
    "            row['label'] = 1 if risk_score > 5.5 else 0\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Add lag of events: previous month events per country\n",
    "    df = df.sort_values([\"country\", \"date\"])\n",
    "    df[\"prev_events\"] = df.groupby(\"country\")[\"label\"].shift(1).fillna(0).astype(int)\n",
    "    \n",
    "    # Create a log_gdp feature to stabilize scale\n",
    "    df[\"log_gdp\"] = np.log1p(df[\"gdp\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    feature_cols = ['log_gdp', 'unemployment', 'youth_pct', 'internet_pct', 'polity', 'prev_events']\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"label\"].copy()\n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Try to load data from files, otherwise generate it\n",
    "try:\n",
    "    X_train = pd.read_csv('../data/X_train.csv')\n",
    "    X_test = pd.read_csv('../data/X_test.csv')\n",
    "    y_train = pd.read_csv('../data/y_train.csv').iloc[:, 0]\n",
    "    y_test = pd.read_csv('../data/y_test.csv').iloc[:, 0]\n",
    "    scaler = joblib.load('../models/scaler.joblib')\n",
    "    print(\"Loaded data from CSV files\")\n",
    "    \n",
    "    # Check if dimensions match\n",
    "    if len(X_train) != len(y_train) or len(X_test) != len(y_test):\n",
    "        print(\"Warning: Data dimensions don't match. Regenerating data...\")\n",
    "        raise FileNotFoundError(\"Data dimensions inconsistent\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Data files not found or inconsistent. Generating and processing synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    df = generate_synthetic_data()\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, feature_cols = prepare_features(df)\n",
    "    \n",
    "    # Split into training and testing sets (time-based split)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    latest_date = df[\"date\"].max()\n",
    "    test_start = latest_date - pd.DateOffset(months=12)\n",
    "    train_mask = df[\"date\"] < test_start\n",
    "    test_mask = df[\"date\"] >= test_start\n",
    "    \n",
    "    X_train, X_test = X[train_mask.values], X[test_mask.values]\n",
    "    y_train, y_test = y[train_mask.values], y[test_mask.values]\n",
    "    \n",
    "    # Handle class imbalance if needed\n",
    "    if y_train.mean() > 0.8:  # If we have too many positive cases\n",
    "        print(\"Downsampling majority class...\")\n",
    "        from sklearn.utils import resample\n",
    "        \n",
    "        # Combine X and y\n",
    "        data = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        # Separate majority and minority classes\n",
    "        majority = data[data[\"label\"] == 1]\n",
    "        minority = data[data[\"label\"] == 0]\n",
    "        \n",
    "        if len(minority) > 0:\n",
    "            # Downsample majority class\n",
    "            majority_downsampled = resample(majority,\n",
    "                                            replace=False,\n",
    "                                            n_samples=len(minority),\n",
    "                                            random_state=42)\n",
    "            \n",
    "            # Combine minority class with downsampled majority class\n",
    "            data_balanced = pd.concat([majority_downsampled, minority])\n",
    "            \n",
    "            # Split back into X and y\n",
    "            X_train = data_balanced[feature_cols]\n",
    "            y_train = data_balanced[\"label\"]\n",
    "        else:\n",
    "            print(\"No minority class examples found. Creating some synthetic negative examples.\")\n",
    "            # Create a few negative samples by slightly modifying positive samples\n",
    "            X_neg = X_train.sample(n=min(50, len(X_train)), random_state=42).copy()\n",
    "            for col in X_train.columns:\n",
    "                if col != 'prev_events':\n",
    "                    X_neg[col] = X_neg[col] * (1 + np.random.normal(0, 0.1, size=len(X_neg)))\n",
    "            y_neg = pd.Series([0] * len(X_neg))\n",
    "            \n",
    "            # Add to original data\n",
    "            X_train = pd.concat([X_train, X_neg])\n",
    "            y_train = pd.concat([y_train, y_neg])\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_cols)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=feature_cols)\n",
    "    \n",
    "    print(\"Generated and processed synthetic data\")\n",
    "\n",
    "# Ensure dimensions match\n",
    "print(f\"X_train shape: {X_train.shape}, y_train length: {len(y_train)}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test length: {len(y_test)}\")\n",
    "\n",
    "if len(X_train) != len(y_train):\n",
    "    print(\"Fixing inconsistent training data dimensions...\")\n",
    "    min_length = min(len(X_train), len(y_train))\n",
    "    X_train = X_train.iloc[:min_length]\n",
    "    y_train = y_train.iloc[:min_length]\n",
    "\n",
    "if len(X_test) != len(y_test):\n",
    "    print(\"Fixing inconsistent test data dimensions...\")\n",
    "    min_length = min(len(X_test), len(y_test))\n",
    "    X_test = X_test.iloc[:min_length]\n",
    "    y_test = y_test.iloc[:min_length]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Class distribution in test set: {pd.Series(y_test).value_counts().to_dict()}\")\n",
    "\n",
    "# ## Model 1: Logistic Regression\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "\n",
    "# Initialize and train model\n",
    "logreg = LogisticRegression(\n",
    "    random_state=42, \n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "y_proba_logreg = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_logreg))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_logreg))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_logreg))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_logreg))\n",
    "print(\"PR AUC:\", average_precision_score(y_test, y_proba_logreg))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_logreg)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(logreg, X_test, y_test)\n",
    "plt.title('Logistic Regression - ROC Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "PrecisionRecallDisplay.from_estimator(logreg, X_test, y_test)\n",
    "plt.title('Logistic Regression - Precision-Recall Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(logreg, '../models/logistic_regression.joblib')\n",
    "print(\"Logistic Regression model saved!\")\n",
    "\n",
    "# ## Model 2: Random Forest\n",
    "\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "\n",
    "# Initialize and train model\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100,\n",
    "    max_depth=10\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
    "print(\"PR AUC:\", average_precision_score(y_test, y_proba_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(rf, X_test, y_test)\n",
    "plt.title('Random Forest - ROC Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "PrecisionRecallDisplay.from_estimator(rf, X_test, y_test)\n",
    "plt.title('Random Forest - Precision-Recall Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Random Forest - Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf, '../models/random_forest.joblib')\n",
    "print(\"Random Forest model saved!\")\n",
    "\n",
    "# ## Model Comparison\n",
    "\n",
    "# Create comparison dataframe\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "accuracy = [accuracy_score(y_test, y_pred_logreg), accuracy_score(y_test, y_pred_rf)]\n",
    "precision = [precision_score(y_test, y_pred_logreg), precision_score(y_test, y_pred_rf)]\n",
    "recall = [recall_score(y_test, y_pred_logreg), recall_score(y_test, y_pred_rf)]\n",
    "f1 = [f1_score(y_test, y_pred_logreg), f1_score(y_test, y_pred_rf)]\n",
    "roc_auc = [roc_auc_score(y_test, y_proba_logreg), roc_auc_score(y_test, y_proba_rf)]\n",
    "pr_auc = [average_precision_score(y_test, y_proba_logreg), average_precision_score(y_test, y_proba_rf)]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'PR AUC': pr_auc\n",
    "}).set_index('Model')\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visual comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'PR AUC']\n",
    "comparison_df[metrics].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Comparison - Performance Metrics')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ## Select Best Model\n",
    "\n",
    "# Based on PR AUC (most important for imbalanced classification)\n",
    "best_model_name = comparison_df['PR AUC'].idxmax()\n",
    "best_model = logreg if best_model_name == 'Logistic Regression' else rf\n",
    "\n",
    "print(f\"Best model based on PR AUC: {best_model_name}\")\n",
    "print(f\"PR AUC: {comparison_df.loc[best_model_name, 'PR AUC']:.3f}\")\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, '../models/best_model.joblib')\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "print(\"Best model and scaler saved!\")\n",
    "\n",
    "# Save processed data for the next notebook\n",
    "pd.DataFrame(X_train, columns=X_train.columns).to_csv('../data/X_train.csv', index=False)\n",
    "pd.DataFrame(X_test, columns=X_test.columns).to_csv('../data/X_test.csv', index=False)\n",
    "pd.Series(y_train).to_csv('../data/y_train.csv', index=False)\n",
    "pd.Series(y_test).to_csv('../data/y_test.csv', index=False)\n",
    "print(\"Processed data saved to data/ folder\")\n",
    "\n",
    "print(\"\\nModel development completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
